{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoM7N1knqqeF"
      },
      "source": [
        "\n",
        "##Simulation Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXB3ZSNyqhdG",
        "outputId": "fa7bff17-132c-48fb-d48e-9994602cc7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (4.12.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.43.0)\n",
            "Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULp4Nx4mT6Wj"
      },
      "source": [
        "##Fast Train Unfolding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjDa6MCzT6Wk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.linalg import svd, eig\n",
        "import math\n",
        "import time\n",
        "import scipy.io as sio\n",
        "\n",
        "torch.set_printoptions(precision=5, sci_mode=False)\n",
        "\n",
        "\n",
        "\n",
        "def set_params(n_0= 1, p_s= 1, B= 2, K= 7, P= 4, N= 2,\n",
        "              L= 2, T= 5, epochs= 500, J= 10,\n",
        "              W_is_phase_only= True, W_is_block_diag= True,\n",
        "              batch_size= 50, pga_iters= 500, lr_mu_beta= 0.0001,\n",
        "              lr_lambda= 0.001, binary_thresh= 0.5,\n",
        "              mat_file_path= \"/content/H_1000_2_8_7.mat\", seed= 1,\n",
        "              train_size= 750, valid_size= 250, lambdas_init= 0.0,\n",
        "              gamma= 200 , project_every= 1, l1_target = 1.0, sharpness = 30,\n",
        "              Q = 4, W_is_finite = False):\n",
        "    \"\"\"\n",
        "    This function sets the general parameters of the algorithm.\n",
        "    Used in order to save space in the code when running multiple simulations.\n",
        "    All of these parameters are predifined and can be modified.\n",
        "    :param n_0: noise power\n",
        "    :param p_s: signal power\n",
        "    :param B: number of frequency bins\n",
        "    :param K: number of users to be served\n",
        "    :param P: number of panels in the base station\n",
        "    :param N: number of antennas in each panel\n",
        "    :param L: number of outputs in each panel\n",
        "    :param T: number of inputs to the CPU\n",
        "    :param epochs: number of epochs in the training\n",
        "    :param J: number of iteration to be unfolded , usually 10.\n",
        "    :param batch_size: batch size in the training part.\n",
        "    :param pga_iters: number of iterations in the PGA algorithm to be compared with.\n",
        "    :param lr_mu_beta: learning rate of the optimizer. denoted as eta in the paper.\n",
        "    :param lr_lambda: learning rate of the optimizer for optimzing lambda.\n",
        "    :param binary_thresh: the threshold for setting elements to zero in the projection onto phases.\n",
        "    :param train_size: size of the channel realizations taken to training set.\n",
        "    :param valid_size: size of the channel realizations taken to validation set.\n",
        "    :return: all the parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # General parameters\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    M = P * N  # Total number of antennas in the Base station\n",
        "    Ik = torch.eye(T, dtype=torch.complex128, device = device)\n",
        "    Im = torch.eye(M, dtype=torch.complex128, device = device)\n",
        "    snr = p_s / n_0\n",
        "\n",
        "    # Building the A matrix using the examples given in the paper\n",
        "    # \"Trade-Offs in Decentralized Multi-Antenna Architectures: The WAX Decomposition\"\n",
        "    A = torch.empty((8,5))\n",
        "    A[:5, :] = torch.eye(5)\n",
        "    A[5:, :3] = torch.eye(3)\n",
        "    A[5:, 3:] = 1\n",
        "    A = A.to(dtype=torch.complex128, device = device)\n",
        "\n",
        "    if T == 5*2:\n",
        "        A = torch.block_diag(A,A)\n",
        "\n",
        "    if T == 5*8:\n",
        "        A = torch.block_diag(A,A,A,A,A,A,A,A)\n",
        "\n",
        "    if T == 5*16:\n",
        "        A = torch.block_diag(A,A,A,A,A,A,A,A)\n",
        "        A = torch.block_diag(A,A)\n",
        "\n",
        "    if T == 61: # suitable only for the case where L*P = 100\n",
        "\n",
        "        A = torch.empty((100, 61))\n",
        "        A[ :61, :] = torch.eye(61)\n",
        "        A[61:, :39] = torch.eye(39)\n",
        "        A[61:83, 39:] = torch.eye(22)\n",
        "        A[83:, 39:56] = torch.eye(17)\n",
        "        A[83:88, 56:] = torch.eye(5)\n",
        "        A[88:93, 56:] = torch.eye(5)\n",
        "        A[93:98, 56:] = torch.eye(5)\n",
        "        A[98:, 56:58] = torch.eye(2)\n",
        "        A[98:, 58:60] = torch.eye(2)\n",
        "        A = A.to(dtype=torch.complex128, device = device)\n",
        "\n",
        "    # Creating D to be a mask matrix that will be used to project the matrix W onto a block diagonal matrix.\n",
        "    D = torch.zeros((M, L * P), device = device)\n",
        "    for p in range(P):\n",
        "        p_n, p_l = p * N, p * L\n",
        "        D[p_n:(p_n + N), p_l:(p_l + L)] = torch.ones((N, L))\n",
        "\n",
        "    # Creating W:\n",
        "    torch.manual_seed(seed)\n",
        "    rand_phases = torch.rand(N * L * P, device=device)\n",
        "    W_vec = torch.exp(1j * 2*math.pi * rand_phases).to(dtype=torch.complex128)\n",
        "    W = torch.block_diag(*W_vec.view(P, N, L))\n",
        "    return W, B, K, P, N, M, L, T, snr, A, Ik, Im, D, J, epochs, W_is_block_diag, W_is_phase_only, batch_size, pga_iters, lr_mu_beta, lr_lambda, binary_thresh, train_size, valid_size, lambdas_init, gamma , mat_file_path, project_every, l1_target, device, sharpness, Q, W_is_finite\n",
        "\n",
        "\n",
        "def soft_mag_proj(w, binary_thresh, sharpness):\n",
        "    w = torch.exp(1j*torch.angle(w)) / (1 + torch.exp(-sharpness * (torch.abs(w) - binary_thresh)))\n",
        "    return w\n",
        "\n",
        "\n",
        "def hard_mag_proj(w, binary_thresh):\n",
        "    w = D * torch.exp(1j*torch.angle(w)) * torch.where(torch.abs(w) < binary_thresh, torch.zeros_like(w), torch.ones_like(w))\n",
        "    return w\n",
        "\n",
        "\n",
        "def soft_phase_proj(W, sharpness):\n",
        "    output_phase = torch.zeros_like(W)\n",
        "    for q in range(1, Q + 1):\n",
        "        # Calculate the shifted and normalized sigmoid for each q\n",
        "        sigmoid_output_q = 2*np.pi/(1+torch.exp(-sharpness*(W.angle()-(q-0.5)*2*torch.pi/Q + np.pi))) - np.pi\n",
        "        output_phase += sigmoid_output_q / Q\n",
        "    return output_phase\n",
        "\n",
        "\n",
        "def hard_phase_proj(W):\n",
        "    output_phase = torch.zeros_like(W)\n",
        "    for q in range(0, Q+1):\n",
        "        # Calculate the shifted and normalized step function for each q\n",
        "        step_output_q = torch.where((W.angle() >= (q-0.5)*2*np.pi/Q - np.pi) & (W.angle() < (q+0.5)*2*np.pi/Q - np.pi), q*2*np.pi/Q - np.pi, 0.0)\n",
        "        output_phase += step_output_q\n",
        "    return output_phase\n",
        "\n",
        "\n",
        "class FastTrainUnfolding(nn.Module):\n",
        "    def __init__(self, W, J, forward_path='Unfolding', mu = None, beta = None, cnn_layers = 10):\n",
        "        super(FastTrainUnfolding, self).__init__()\n",
        "        self.J = J\n",
        "        self.W = W.detach().clone()  # Make a copy to avoid modifying the original W\n",
        "        self.ones = torch.ones(1, dtype = torch.float64, device = device)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.pga_iters = pga_iters\n",
        "        self.constant_mu = mu\n",
        "        self.constant_beta = beta\n",
        "        self.cnn_layers = cnn_layers\n",
        "        self.PE = project_every\n",
        "\n",
        "\n",
        "        # Define layers for Unfolded:\n",
        "        self.Mu = nn.ModuleList([nn.Linear(1, P*N*L, dtype = torch.float64) for _ in range(J)])\n",
        "        self.Beta = nn.ModuleList([nn.Linear(1, P*N*L, dtype = torch.float64) for _ in range(J)])\n",
        "        self.Lambda = nn.ModuleList([nn.Linear(1, P*N*L, dtype = torch.float64) for _ in range(J)])\n",
        "\n",
        "        if forward_path == \"CNN\":\n",
        "            # Define layers for the CNN.\n",
        "            mag_layers = []\n",
        "            for _ in range(self.cnn_layers - 1):\n",
        "              mag_layers.extend([nn.Conv2d(B, B, kernel_size = 4, padding = 2, stride = 1, dtype = torch.float64),\n",
        "                                nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "                                nn.Flatten(),\n",
        "                                nn.Linear(((M+1)//2 * ((K+1)//2) * B ), B*M*K, dtype = torch.float64),\n",
        "                                nn.Unflatten(-1,(B,M,K)),\n",
        "                                nn.BatchNorm2d(B, dtype = torch.float64),\n",
        "                                ])\n",
        "            self.cnn_mag = nn.Sequential(*mag_layers)\n",
        "\n",
        "            phase_layers = []\n",
        "            for _ in range(self.cnn_layers - 1):\n",
        "              phase_layers.extend([nn.Conv2d(B, B, kernel_size = 4, padding = 2, stride = 1, dtype = torch.float64),\n",
        "                                nn.BatchNorm2d(B, dtype = torch.float64),\n",
        "                                nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "                                nn.Flatten(),\n",
        "                                nn.Linear(((M+1)//2 * ((K+1)//2) * B), B*M*K, dtype = torch.float64),\n",
        "                                nn.Unflatten(-1,(B,M,K))\n",
        "                                ])\n",
        "            self.cnn_phase = nn.Sequential(*phase_layers)\n",
        "\n",
        "\n",
        "            self.cnn_f_layer = nn.Linear(2 * B * M * K, N * P * L, dtype = torch.float64)\n",
        "\n",
        "        # Define the selected forward path\n",
        "        if forward_path == 'Unfolding':\n",
        "            self.forward_path = self.forward_unfolded\n",
        "        elif forward_path == 'Constant':\n",
        "            self.forward_path = self.forward_constant\n",
        "        elif forward_path == \"CNN\":\n",
        "            self.forward_path = self.forward_cnn\n",
        "        else:\n",
        "            raise ValueError(\"Invalid forward_path. Choose 'Unfolding' or 'Constant'.\")\n",
        "\n",
        "\n",
        "    def forward(self, H):\n",
        "        return self.forward_path(H)\n",
        "\n",
        "\n",
        "    def forward_cnn(self, H):\n",
        "        # Convert complex number to magnitude and phase\n",
        "        H_mag, H_phase = torch.abs(H), torch.angle(H)\n",
        "\n",
        "        # Pass through the neural network layers\n",
        "        output_mag = self.cnn_mag(H_mag).view(H_mag.size(0), -1)\n",
        "        output_phase = self.cnn_phase(H_phase).view(H_phase.size(0), -1)\n",
        "        final_vec = torch.cat((output_mag, output_phase), dim=1)\n",
        "\n",
        "        # Convert magnitude and phase back to complex number\n",
        "        W_phases_vec = self.cnn_f_layer(final_vec)\n",
        "        # if W_is_finite:\n",
        "        #     W_phases_vec = hard_phase_proj(W_phases_vec)\n",
        "        W_phases_blocks = W_phases_vec.view(-1,P,N,L)\n",
        "        W_pred = D * torch.exp(1j * 2*math.pi * vec_2_blk_diag(W_phases_vec, batch=True))\n",
        "        cnn_R = self.calc_R(H,W_pred)\n",
        "        return W_pred, cnn_R\n",
        "\n",
        "\n",
        "    def forward_unfolded(self, H):\n",
        "        W_1 = self.W.clone()\n",
        "        Rs = torch.zeros(H.shape[0], J)\n",
        "        l1_norms = torch.zeros(H.shape[0], J)\n",
        "        W_0 = torch.zeros_like(W_1)\n",
        "        for j in range(self.J):\n",
        "            grad = self.calc_dR_dW(H, W_1)\n",
        "            weights = self.Mu[j](self.ones)\n",
        "            Mu_step = vec_2_blk_diag(weights) * grad\n",
        "            Momentum = vec_2_blk_diag(self.Beta[j](self.ones)) * (W_1 - W_0)\n",
        "            norm_reduc = 0.001*vec_2_blk_diag(self.Lambda[j](self.ones)) * torch.exp(1j*torch.angle(W_1))\n",
        "            ## Updating:\n",
        "            W_2 = W_1 + Mu_step + Momentum  - norm_reduc\n",
        "\n",
        "            ## Projecting:\n",
        "            ## If training, use Soft Projecting:\n",
        "            if (j+1) % self.PE == 0 and self.training:\n",
        "                W_2 = D * soft_mag_proj(W_2, binary_thresh = binary_thresh, sharpness = sharpness)\n",
        "                if W_is_finite:\n",
        "                    W_2 = D * torch.exp(1j*soft_phase_proj(W_2, sharpness = sharpness))\n",
        "\n",
        "            ## If Validating, use Hard Projecting:\n",
        "            if (j+1) % self.PE == 0 and not self.training:\n",
        "                W_2 = hard_mag_proj(W_2, binary_thresh = binary_thresh)\n",
        "                if W_is_finite:\n",
        "                    W_2 = D * torch.exp(1j*hard_phase_proj(W_2))\n",
        "\n",
        "\n",
        "            ## Documenting:\n",
        "            l1_norms[:,j] = torch.norm(W_2, p=1)\n",
        "            Rs[:,j] = self.calc_R(H, W_2)\n",
        "            W_0 = W_1\n",
        "            W_1 = W_2\n",
        "\n",
        "        return W_2, Rs, l1_norms\n",
        "\n",
        "\n",
        "    def forward_constant(self, H):\n",
        "        W_1 = self.W.clone()\n",
        "        W_0 = torch.zeros_like(W_1)\n",
        "        Rs = torch.zeros(H.shape[0], self.pga_iters)\n",
        "        for iter in range(self.pga_iters):\n",
        "            grad = self.calc_dR_dW(H, W_1)\n",
        "            Mu_step = self.constant_mu * grad\n",
        "            Momentum = self.constant_beta * (W_1 - W_0)\n",
        "            ## Updating:\n",
        "            W_2 = W_1 + Mu_step + Momentum\n",
        "            ## Projecting:\n",
        "            if (iter+1) % self.PE == 0:\n",
        "                W_2 = D * (torch.exp(1j * torch.angle(W_2)))\n",
        "                if W_is_finite:\n",
        "                    W_2 = D * torch.exp(1j*hard_phase_proj(W_2))\n",
        "\n",
        "            ## Documenting:\n",
        "            Rs[:,iter] = self.calc_R(H, W_2)\n",
        "            W_0 = W_1\n",
        "            W_1 = W_2\n",
        "        return W_2, Rs\n",
        "\n",
        "\n",
        "\n",
        "    def calc_dR_dW(self, H, W):\n",
        "        \"\"\"calculates the gradient of the rate with respect to W. the gradient has the same dimension as W\"\"\"\n",
        "        grad = torch.zeros(H.size(0), M, L*P, dtype = torch.complex128, device =self.device)\n",
        "        for b in range(B):\n",
        "            h1 = H[:,b, :, :]\n",
        "            W_con = W.conj()\n",
        "            H_con = h1.conj()\n",
        "            # print(W.shape)\n",
        "            # print(A.shape)\n",
        "            G = W @ A\n",
        "            if len(G.shape) < 3:\n",
        "              G = G.unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
        "            G_herm = G.transpose(1, 2).conj()\n",
        "            h1_herm = h1.transpose(1, 2).conj()\n",
        "            G_psu_inv = torch.inverse(G_herm @ G + 10**-6 * torch.eye(T, device = self.device))\n",
        "            Proj_mat = G @ G_psu_inv @ G_herm\n",
        "            Z = snr * G_psu_inv @ G_herm @ h1 @ h1_herm @ G\n",
        "            dR_dZ = torch.inverse(Ik + Z)\n",
        "            diff = snr * (A @ dR_dZ @ (2 * G_psu_inv @ G_herm) @ h1 @ h1_herm @ (Im - Proj_mat)).conj().transpose(1,2)\n",
        "            grad += diff\n",
        "        return grad / B\n",
        "\n",
        "\n",
        "    def calc_dR_dW2(self, H, W):\n",
        "        \"\"\"calculates the gradient of the rate with respect to W. the gradient has the same dimension as W\"\"\"\n",
        "        grad = torch.zeros(H.size(0), M, L*P, dtype = torch.complex128, device =self.device)\n",
        "        for b in range(B):\n",
        "            h1 = H[:,b, :, :]\n",
        "            W_con = W.conj()\n",
        "            H_con = h1.conj()\n",
        "            # print(W.shape)\n",
        "            # print(A.shape)\n",
        "            G = W @ A\n",
        "            if len(G.shape) < 3:\n",
        "              G = G.unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
        "            G_herm = G.transpose(1, 2).conj()\n",
        "            h1_herm = h1.transpose(1, 2).conj()\n",
        "            G_psu_inv = torch.inverse(G_herm @ G + 10**-10 * torch.eye(T, device = self.device)) @ G_herm\n",
        "            Proj_mat = G @ G_psu_inv\n",
        "            Z = snr * G_psu_inv @ h1 @ h1_herm @ G\n",
        "            dR_dZ = torch.inverse(Ik + Z)\n",
        "            diff = 2 * snr * (A @ dR_dZ @ G_psu_inv  @ h1 @ h1_herm @ (Im - Proj_mat)).conj().transpose(1,2)\n",
        "            grad += diff\n",
        "        return grad / B\n",
        "\n",
        "\n",
        "    def calc_dR_dW3(self, H, W):\n",
        "            \"\"\"calculates the gradient of the rate with respect to W. the gradient has the same dimension as W\"\"\"\n",
        "            grad = torch.zeros(H.size(0), M, L*P, dtype = torch.complex128, device =self.device)\n",
        "            for b in range(B):\n",
        "                h1 = H[:,b, :, :]\n",
        "                W_con = W.conj()\n",
        "                H_con = h1.conj()\n",
        "                G = W @ A\n",
        "                if len(G.shape) < 3:\n",
        "                  G = G.unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
        "                G_herm = G.transpose(1, 2).conj()\n",
        "                h1_herm = h1.transpose(1, 2).conj()\n",
        "                G_psu_inv = torch.linalg.inv(G_herm @ G + 10**-10 * torch.eye(T, device = self.device)) @ G_herm\n",
        "                Proj_mat = G @ G_psu_inv\n",
        "                G_pinv_X_h_X_h_herm = G_psu_inv @ h1 @ h1_herm\n",
        "                Z = snr * G_pinv_X_h_X_h_herm @ G\n",
        "                dR_dZ = torch.linalg.inv(Ik + Z)\n",
        "                diff = 2 * snr * (A @ dR_dZ @ G_pinv_X_h_X_h_herm @ (Im - Proj_mat)).conj().transpose(1,2)\n",
        "                grad += diff\n",
        "            return grad / B\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def calc_R(self, H, W):\n",
        "        R = 0\n",
        "        G = W @ A\n",
        "        G_herm = G.transpose(1, 2).conj()\n",
        "        for b in range(B):\n",
        "          h1 = H[:,b,:,:]\n",
        "          h1_herm = h1.transpose(1, 2).conj()\n",
        "          psudo_inv_G = torch.inverse(G_herm @ G + 10**-6 * torch.eye(T,device = self.device))\n",
        "          proj_mat = G @ psudo_inv_G @ G_herm\n",
        "          R += torch.log((Im + snr * proj_mat @ h1 @ h1_herm).det())\n",
        "        return R/B\n",
        "\n",
        "\n",
        "    def double_weights(self):\n",
        "        # Double the weights\n",
        "        for module_list in [self.Mu, self.Beta, self.Lambda]:\n",
        "            for module in module_list:\n",
        "                new_weights = torch.cat((module.weight, module.weight.clone()), dim=0).detach()\n",
        "                module.weight = torch.nn.Parameter(new_weights)\n",
        "\n",
        "                 # Double bias (if bias exists)\n",
        "                if module.bias is not None:\n",
        "                    new_bias = torch.cat((module.bias, module.bias.clone()), dim=0).detach()\n",
        "                    module.bias = torch.nn.Parameter(new_bias)\n",
        "\n",
        "\n",
        "\n",
        "class QuaDRiGA(Dataset):\n",
        "    def __init__(self):\n",
        "        self.num_samples = valid_size + train_size\n",
        "        self.H = sio.loadmat(mat_file_path)['H']\n",
        "        self.H = torch.tensor(self.H.transpose(0, 3, 2, 1)).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.H[idx,:B,:,:]\n",
        "        return sample\n",
        "\n",
        "\n",
        "class IID(Dataset):\n",
        "    def __init__(self):\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.num_samples = valid_size + train_size\n",
        "        self.H = torch.randn((self.num_samples,B,M,K), dtype = torch.complex128, device = device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.H[idx,:,:,:]\n",
        "        return sample\n",
        "\n",
        "\n",
        "def vec_2_blk_diag(vec, batch=False):\n",
        "    if batch:\n",
        "        size_batch = vec.shape[0]\n",
        "        assert len(vec.shape) == 2, \"Invalid shape for batched vector\"\n",
        "        assert vec.shape[1] == P * N * L, \"Invalid vector length for batched vector\"\n",
        "        blocks = vec.view(size_batch, P, N, L)\n",
        "    else:\n",
        "        assert len(vec.shape) == 1, \"Invalid shape for non-batched vector\"\n",
        "        assert vec.shape[0] == P * N * L, \"Invalid vector length for non-batched vector\"\n",
        "        blocks = vec.view(P, N, L)\n",
        "\n",
        "    if batch:\n",
        "        result_matrix = torch.cat([torch.block_diag(*blocks[i].unbind(0)).unsqueeze(0) for i in range(size_batch)], dim=0)\n",
        "    else:\n",
        "        result_matrix = torch.block_diag(*blocks.unbind(0)).unsqueeze(0)\n",
        "\n",
        "    return result_matrix\n",
        "\n",
        "\n",
        "def train_and_plot(model, num_epochs, train_loader, val_loader, cnn = False, seq_learning = False, pre_model= None, plot = True):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    l1_norms_list_train = []\n",
        "    l1_norms_list_val = []\n",
        "    log_weights = torch.log(torch.arange(2, J + 2))\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        total_l1_norms = 0.0\n",
        "        for H_batch in train_loader:\n",
        "            if not cnn: # If we are training the Unfolded network\n",
        "                u_optimizer.zero_grad()\n",
        "                lam_optimizer.zero_grad()\n",
        "                if seq_learning:\n",
        "                    W_out_pre,Rs_pre = pre_model(H_batch)\n",
        "                    model.W = W_out_pre\n",
        "                W_out_u, Rs, l1_norms = model(H_batch)\n",
        "                l1_norms = l1_norms.mean(dim=0) / (N*P*L*(Rs.shape[0])) # averaging on them\n",
        "                Rs = Rs.mean(dim = 0) # averaging on them\n",
        "\n",
        "                loss = -torch.sum(Rs * log_weights) # Taking the last value of R for each sample\n",
        "                loss += gamma * (torch.sum(log_weights*(l1_norms - l1_target)**2))\n",
        "                loss.backward()\n",
        "                u_optimizer.step()\n",
        "                lam_optimizer.step()\n",
        "\n",
        "                total_train_loss += Rs[-1].item() / K\n",
        "                total_l1_norms += l1_norms[-1].item()\n",
        "\n",
        "\n",
        "            else: # If we are training the CNN network\n",
        "                cnn_optimizer.zero_grad()\n",
        "                W_out_cnn, cnn_R = model(H_batch)\n",
        "                if seq_learning: # Learning a sequence of networks\n",
        "                    pre_model.W = W_out_cnn\n",
        "                    W_out_post, Rs_post, _ = pre_model(H_batch)\n",
        "                    loss = -torch.sum(Rs_post.mean(dim=0) * log_weights) # Taking the last value of R for each sample\n",
        "                    loss.backward()\n",
        "                    cnn_optimizer.step()\n",
        "                    total_train_loss += Rs_post.mean(dim=0)[-1].item() / K\n",
        "\n",
        "                else: # Regular Learning of CNN\n",
        "                    cnn_R = cnn_R.mean(dim = 0) # averaging on them\n",
        "                    loss = -cnn_R\n",
        "                    loss.backward()\n",
        "                    cnn_optimizer.step()\n",
        "                    total_train_loss += cnn_R.item() / K\n",
        "\n",
        "\n",
        "        average_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(average_train_loss)\n",
        "        if not cnn:\n",
        "          l1_norms_list_train.append(total_l1_norms / len(train_loader))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            total_val_loss = 0.0\n",
        "            val_rate = 0.0\n",
        "            total_l1_norms_v = 0.0\n",
        "            for H_val_batch in val_loader:\n",
        "                if not cnn: # If we are training the Unfolded network\n",
        "                    if seq_learning:\n",
        "                        W_out_pre,Rs_pre = pre_model(H_val_batch)\n",
        "                        model.W = W_out_pre\n",
        "                    W_val_pred, Rs_val, l1_norms_v = model(H_val_batch)\n",
        "                    l1_norms_v = l1_norms_v.mean(dim=0) / (P*N*L*(Rs_val.shape[0]))\n",
        "                    Rs_val = Rs_val.mean(dim=0) / K\n",
        "                    val_rate += Rs_val\n",
        "                    total_val_loss += Rs_val[-1].item()\n",
        "                    total_l1_norms_v += l1_norms_v[-1].item()\n",
        "                else: # If we are training the CNN network\n",
        "                    W_val_pred, cnn_R_val = model(H_val_batch)\n",
        "                    if seq_learning:\n",
        "                        pre_model.W = W_val_pred\n",
        "                        W_val_post,Rs_post,_ = pre_model(H_val_batch)\n",
        "                        cnn_R_val = Rs_post.mean(dim=0)[-1] / K\n",
        "                    else:\n",
        "                        cnn_R_val = cnn_R_val.mean(dim=0) / K\n",
        "                    val_rate += cnn_R_val\n",
        "                    total_val_loss += cnn_R_val.item()\n",
        "\n",
        "            average_val_loss = total_val_loss / len(val_loader)\n",
        "            val_rate = val_rate / len(val_loader)\n",
        "            val_losses.append(average_val_loss)\n",
        "            if not cnn:\n",
        "                l1_norms_list_val.append(total_l1_norms_v / len(val_loader))\n",
        "\n",
        "\n",
        "\n",
        "        # Print the loss for each epoch\n",
        "        if (epoch +1) % 50 == 0:\n",
        "          print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
        "                f'Training: {round(abs(average_train_loss), 5)}, '\n",
        "                f'Validation: {round(abs(average_val_loss), 5)}')\n",
        "\n",
        "    if plot:\n",
        "        # Plot the training and validation losses\n",
        "        plt.plot(train_losses, label='Train')\n",
        "        plt.plot(val_losses, label='Valid')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Channel Rate')\n",
        "        plt.title('CNN Training' if cnn else 'Unfolding Training')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "    return val_rate, model, l1_norms_list_val, l1_norms_list_train\n",
        "\n",
        "\n",
        "def constant_mu_and_beta_search():\n",
        "  mu_grid = torch.cat((torch.linspace(0.01,0.5,6),torch.linspace(1,5,5)))\n",
        "  beta_grid = torch.cat((torch.linspace(0,0.99,3),torch.tensor([0.9])))\n",
        "  search_Rs = 0\n",
        "  best_Rs = torch.zeros(pga_iters)\n",
        "  best_mu = 0\n",
        "  best_beta = 0\n",
        "  indices = np.arange(project_every, pga_iters +1, project_every)\n",
        "  for mu in mu_grid:\n",
        "      for beta in beta_grid:\n",
        "        con_model = FastTrainUnfolding(W, J, \"Constant\", mu, beta)\n",
        "        search_Rs = 0\n",
        "        for H_val_batch in val_loader:\n",
        "          W_val_pred, Rs_val = con_model(H_val_batch)\n",
        "          Rs_val = Rs_val.mean(dim=0) / K\n",
        "          search_Rs += Rs_val\n",
        "        search_Rs = search_Rs / len(val_loader)\n",
        "        if max(abs(search_Rs[indices-1])) > max(abs(best_Rs[indices-1])):\n",
        "          best_Rs = search_Rs\n",
        "          best_mu = mu\n",
        "          best_beta = beta\n",
        "          # print(f' Updated, mu: {round(mu.item(),2)}, beta: {round(beta.item(),2)}, R: {round(max(abs(best_Rs)).item(),6)}')\n",
        "\n",
        "  print(f'best mu: {round(best_mu.item(),2)}, best beta: {round(best_beta.item(),2)}, best R: {round(max(abs(best_Rs)).item(),6)}')\n",
        "  return best_Rs, best_mu, best_beta\n",
        "\n",
        "\n",
        "def plot_R(unfolding_R, best_Rs, ls_Rs, cnn_R, Mo_Rs, alt_learn_Rs= None, full=False):\n",
        "    indices = np.arange(project_every,len(best_Rs)+1, project_every)\n",
        "    u_indices = np.arange(project_every,len(unfolding_R)+1,project_every)\n",
        "    ls_indices = np.arange(project_every,len(ls_Rs)+1, project_every)\n",
        "    mo_indices = np.arange(project_every,len(Mo_Rs)+1, project_every)\n",
        "\n",
        "    plt.plot(u_indices, unfolding_R[u_indices-1],\"-o\", label='U-PGA+M', color = \"blue\")\n",
        "    plt.plot(indices, best_Rs.detach().numpy()[indices-1],\"--^\", label = \"PGA+M\", color = \"orange\")\n",
        "    plt.plot(ls_indices, ls_Rs.detach().numpy()[ls_indices-1],\"--*\", label = \"Line Search\", color = \"green\")\n",
        "    plt.plot(mo_indices, Mo_Rs.detach().numpy()[mo_indices-1],\"-.\", label = \"MO\", color = \"cyan\")\n",
        "    if alt_learn_Rs is not None:\n",
        "          alt_Rs_indices = np.arange(project_every,len(alt_learn_Rs)+1, project_every)\n",
        "          plt.plot(alt_Rs_indices, alt_learn_Rs.detach().numpy()[alt_Rs_indices-1],\"-x\", label = \"CNN + U-PGA+M\", color = \"red\")\n",
        "\n",
        "    plt.axhline(y=abs(cnn_R.item()), color='magenta', linestyle=':', label=\"CNN - {} layers\".format(cnn_model.cnn_layers))\n",
        "    plt.axhline(y=max(abs(best_Rs[indices-1].detach().numpy())), color='black', linestyle='--', label='PGA+M - {} iters'.format(pga_iters))\n",
        "\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(\"Bits per channel use per user\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    if not full:\n",
        "        plt.xlim(project_every,J)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_norms():\n",
        "    plt.plot(range(1,len(u_l1_norms_v)+1),u_l1_norms_v, label=\"Valid\")\n",
        "    plt.plot(range(1,len(u_l1_norms_t)+1),u_l1_norms_t, label= \"Train\")\n",
        "    plt.ylabel('% of components on')\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.axhline(y=l1_target, color='k', linestyle='--', label = \"target\")\n",
        "    plt.grid()\n",
        "    plt.title(\"L1 Norms\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def gen_data(train_size,valid_size,dataset=\"QuaDRiGA\"):\n",
        "  if dataset == \"QuaDRiGA\":\n",
        "    dataset = QuaDRiGA()\n",
        "\n",
        "  if dataset == \"I.I.D.\":\n",
        "      dataset = IID()\n",
        "\n",
        "  # # Split the dataset into training and validation sets\n",
        "  train_dataset = torch.utils.data.Subset(dataset, range(valid_size, train_size + valid_size))\n",
        "  val_dataset = torch.utils.data.Subset(dataset, range(valid_size))\n",
        "\n",
        "  # # Create DataLoaders for training and validation\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_loader_singles = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
        "  return train_loader, val_loader, val_loader_singles\n",
        "\n",
        "\n",
        "def show_mu_and_beta(model):\n",
        "\n",
        "    # Create subplots for Mu\n",
        "    fig, axs_mu = plt.subplots(2, (J + 1) // 2, figsize=(21, 8.5))\n",
        "\n",
        "    # Calculate global min and max for Mu\n",
        "    global_min_mu = min([min(model.Mu[j](model.ones)) for j in range(J)])\n",
        "    global_max_mu = max([max(model.Mu[j](model.ones)) for j in range(J)])\n",
        "\n",
        "    # After training is complete\n",
        "    for j, linear_layer in enumerate(model.Mu):\n",
        "        # Pass ones vector through the linear layer, to see its values\n",
        "        Mu_j = vec_2_blk_diag(model.Mu[j](model.ones))\n",
        "\n",
        "\n",
        "        # Plot magnitude\n",
        "        im1 = axs_mu[j // ((J + 1) // 2), j % ((J + 1) // 2)].imshow(Mu_j.squeeze(0).cpu().detach().numpy(), cmap='viridis', vmin=global_min_mu, vmax=global_max_mu)\n",
        "        axs_mu[j // ((J + 1) // 2), j % ((J + 1) // 2)].set_title(f'Layer {j}')\n",
        "\n",
        "    # Create a colorbar for Mu\n",
        "    cbar_mu_ax = fig.add_axes([0.25, 0.08, 0.5, 0.07])  # [left, bottom, width, height]\n",
        "    cbar_mu = fig.colorbar(im1, cax=cbar_mu_ax, orientation='horizontal')\n",
        "\n",
        "    # Adjust layout for Mu and the colorbar\n",
        "    plt.subplots_adjust(top=0.85, bottom=0.2, right=0.9 if J % 2 == 0 else 0.85)\n",
        "    plt.suptitle('Mu', fontsize=40, y=0.95)\n",
        "    plt.show()\n",
        "\n",
        "    # Create subplots for Beta\n",
        "    fig, axs_beta = plt.subplots(2, (J + 1) // 2, figsize=(21, 8.5))\n",
        "\n",
        "     # Calculate global min and max for Mu\n",
        "    global_min_beta = min([min(model.Beta[j](model.ones)) for j in range(J)])\n",
        "    global_max_beta = max([max(model.Beta[j](model.ones)) for j in range(J)])\n",
        "\n",
        "\n",
        "    # After training is complete\n",
        "    for j, linear_layer in enumerate(model.Beta):\n",
        "        # Pass ones vector through the linear layer, to see its values\n",
        "        Beta_j = vec_2_blk_diag(model.Beta[j](model.ones))\n",
        "\n",
        "        # Plot magnitude\n",
        "        im1 = axs_beta[j // ((J + 1) // 2), j % ((J + 1) // 2)].imshow(Beta_j.squeeze(0).cpu().detach().numpy(), cmap='viridis', vmin=global_min_beta, vmax=global_max_beta)\n",
        "        axs_beta[j // ((J + 1) // 2), j % ((J + 1) // 2)].set_title(f'Layer {j}')\n",
        "\n",
        "    # Create a colorbar for Beta\n",
        "    cbar_beta_ax = fig.add_axes([0.25, 0.08, 0.5, 0.07])  # [left, bottom, width, height]\n",
        "    cbar_beta = fig.colorbar(im1, cax=cbar_beta_ax, orientation='horizontal')\n",
        "\n",
        "    # Adjust layout for Beta and the colorbar\n",
        "    plt.subplots_adjust(top=0.85, bottom=0.2, right=0.9 if J % 2 == 0 else 0.85)\n",
        "    plt.suptitle('Beta', fontsize=40, y=0.95)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def line_search(ls_iters):\n",
        "    \"\"\" Iterates over every sample in the validation set, and finds the best step size for each iteration, for each sample. \"\"\"\n",
        "    # Initializing the PGA + M object with default values:\n",
        "    ls_model = FastTrainUnfolding(W, J, \"Constant\")\n",
        "    best_steps = torch.zeros((len(val_loader_sin)), ls_iters)\n",
        "    best_Rs = torch.zeros((len(val_loader_sin)), ls_iters)\n",
        "    grid = torch.cat((torch.linspace(0.01,0.5,6),torch.linspace(1,5,5)))\n",
        "    # Takes a single sample:\n",
        "    for idx, sample in enumerate(val_loader_sin):\n",
        "        W_ls = W.detach().clone()\n",
        "        sample.unsqueeze(0)\n",
        "        # Start moving over all the iterations in \"line_search_iters\".\n",
        "        for iteration in range(ls_iters):\n",
        "            # initianting the best step and the best R to be 0:\n",
        "            best_step = 0\n",
        "            best_R = 0\n",
        "            # Calculating the gradient:\n",
        "            grad = ls_model.calc_dR_dW(sample, W_ls)\n",
        "            # Looking for the best step size with the calculated gradient:\n",
        "            for step in grid:\n",
        "                # Updating W with the step size:\n",
        "                W_temp = W_ls + step * grad\n",
        "                if (iteration + 1) % project_every == 0:\n",
        "                    W_temp = hard_mag_proj(W_temp,binary_thresh=0)\n",
        "                    if W_is_finite:\n",
        "                      W_temp = D * torch.exp(1j*hard_phase_proj(W_temp))\n",
        "\n",
        "                # Calculating the optional R.\n",
        "                R = ls_model.calc_R(sample, W_temp)\n",
        "                if abs(R) > abs(best_R): # If the calculated R is better than the best R so far, save the step size.\n",
        "                    best_R = R\n",
        "                    best_step = step\n",
        "            # Before the next iteration, update W with the best step chosen.\n",
        "            W_ls = W_ls + best_step * grad\n",
        "            if (iteration + 1) % project_every == 0:\n",
        "                W_ls = hard_mag_proj(W_ls,binary_thresh= 0)\n",
        "                if W_is_finite:\n",
        "                    W_ls = D * torch.exp(1j*hard_phase_proj(W_ls))\n",
        "            best_steps[idx,iteration] = best_step # saving the best step and the best R.\n",
        "            best_Rs[idx,iteration] = best_R\n",
        "    average_best_Rs = torch.mean(best_Rs, dim=0)  # Calculate the mean R\n",
        "    average_best_steps = torch.mean(best_steps, dim=0)  # Calculate the mean best step size, for comparison reasons.\n",
        "    return average_best_Rs / K, average_best_steps\n",
        "\n",
        "\n",
        "def manifold_optimization(MO_iters):\n",
        "    \"\"\" Iterates over every sample in the validation set, and finds the best step size for each iteration,\n",
        "     following the objective of approching G_opt, achieved by SVD. \"\"\"\n",
        "    # Initializing the PGA + M object with defatult values:\n",
        "    mo_model = FastTrainUnfolding(W, J, \"Constant\")\n",
        "    best_steps = torch.zeros((len(val_loader_sin)), MO_iters)\n",
        "    best_Rs = torch.zeros((len(val_loader_sin)), MO_iters)\n",
        "    grid = torch.cat((torch.linspace(0.01,0.5,3),torch.linspace(1,5,3)))\n",
        "    W_init = W.detach().clone()\n",
        "    E = torch.zeros((M, T), dtype=torch.complex128, device = device)\n",
        "    E[:T, :T] = torch.eye(T)\n",
        "    # Takes a single sample:\n",
        "    for idx, sample in enumerate(val_loader_sin):\n",
        "        W_0 = W_init.detach().clone().unsqueeze(0)\n",
        "        prev_grad = torch.zeros_like(W_init)\n",
        "        # Start moving over all the iterations in \"MO_iters\" (Usually 50).\n",
        "        for iteration in range(MO_iters):\n",
        "            # initianting the best step and the best R to be 0:\n",
        "            best_step = 0\n",
        "            best_R = 0\n",
        "            min_error = 10**8  # taking a high number, to be used as \"inifinity\" for initilizaing the algorithm\n",
        "\n",
        "            # Calculates the gradient:\n",
        "            H_H_herm = sample @ (sample.conj().transpose(2,3))\n",
        "            eigenvalues, eigenvectors = eig(H_H_herm)\n",
        "            # Arrange eigenvectors in descending order\n",
        "            sorted_indices = torch.argsort(abs(eigenvalues), descending=True)\n",
        "            F = torch.stack([(eigenvectors[0,b,:,:])[sorted_indices[0,b]] for b in range(B)])\n",
        "            G_opt = F @ E\n",
        "            G_opt = torch.mean(G_opt, dim=0)\n",
        "            grad = -2*(G_opt - W_0 @ A) @ A.transpose(0,1)\n",
        "\n",
        "            #  Momentum:\n",
        "            if iteration > 1 :\n",
        "                # Using Polak-Ribiere momentum factor\n",
        "                beta = grad * (grad - prev_grad) / (torch.norm(prev_grad)**2 + 10**-6)\n",
        "                grad += beta * prev_grad\n",
        "\n",
        "            # Calculating the inner product between the grad and the conjugate matrix:\n",
        "            inner_product = (torch.real(grad * W_0.conj()))\n",
        "            # Looking for the best step size with the calculated gradient:\n",
        "            for step in grid:\n",
        "                # Updating W with the step size:\n",
        "                # Calculating the inner product between the grad and the conjugate matrix\n",
        "                W_temp = W_0 + step * (grad - inner_product * W_0)\n",
        "                if (iteration + 1) % project_every == 0:\n",
        "                    W_temp = hard_mag_proj(W_temp,binary_thresh=0)\n",
        "                    if W_is_finite:\n",
        "                      W_temp = D * torch.exp(1j*hard_phase_proj(W_temp))\n",
        "\n",
        "                # Calculating the optional error.\n",
        "                curr_error = torch.norm(G_opt - W_temp @ A)\n",
        "                if abs(curr_error) < abs(min_error): # If the calculated R is better than the best R so far, save the step size.\n",
        "                    best_R = mo_model.calc_R(sample, W_temp)\n",
        "                    best_step = step\n",
        "                    min_error = abs(curr_error)\n",
        "\n",
        "            # Before the next iteration, update W with the best step chosen.\n",
        "            W_0 = W_0 + best_step * (grad - inner_product * W_0)\n",
        "            if (iteration + 1) % project_every == 0:\n",
        "                W_0 = hard_mag_proj(W_0,binary_thresh=0)\n",
        "\n",
        "            #saving previuos to calculate the next beta according to Polak Ribiere:\n",
        "            prev_grad = grad\n",
        "            best_steps[idx,iteration] = best_step # saving the best step and the best R.\n",
        "            best_Rs[idx,iteration] = best_R\n",
        "\n",
        "    average_best_Rs = torch.mean(best_Rs, dim=0)  # Calculate the mean R\n",
        "    average_best_steps = torch.mean(best_steps, dim=0)  # Calculate the mean best step size, for comparison reasons.\n",
        "    return average_best_Rs / K, average_best_steps\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMEjdzDRK_rs"
      },
      "source": [
        "##Running times: (On CPU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking a single batch, of 10,000 samples."
      ],
      "metadata": {
        "id": "4uZJIXyGQWjE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJbmeOjfVKDP"
      },
      "source": [
        "##Scenario 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JckW_6jXVKDP"
      },
      "outputs": [],
      "source": [
        "# # Setting parameters\n",
        "W, B, K, P, N, M, L, T, snr, A, Ik, Im, D, J,\\\n",
        "epochs, W_is_block_diag, W_is_phase_only, batch_size, \\\n",
        "pga_iters, lr_mu_beta, lr_lambda, binary_thresh, train_size,\\\n",
        "valid_size, lambdas_init, gamma ,mat_file_path, project_every,\\\n",
        "l1_target, device, sharpness, Q, W_is_finite \\\n",
        "= set_params(batch_size = 10000, valid_size = 10000,\n",
        "             P=4, L=2, N=3, K=5, B=4, T=5)\n",
        "\n",
        "# # Generate the data\n",
        "train_loader, val_loader, val_loader_sin = gen_data(train_size, valid_size, \"I.I.D.\")\n",
        "u_model = FastTrainUnfolding(W,J,\"Unfolding\").to(device)\n",
        "u_model.eval()\n",
        "\n",
        "const_model = FastTrainUnfolding(W,J,\"Constant\", mu = torch.randn(1),beta = torch.randn(1)).to(device)\n",
        "const_model.eval()\n",
        "\n",
        "cnn_model = FastTrainUnfolding(W,J,\"CNN\", cnn_layers = 10).to(device)\n",
        "cnn_model.eval()\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6j8p8_cVKDP",
        "outputId": "f4d27c98-b754-4616-feac-490296dcf1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN 10 layers inference time:  290 us\n",
            "U - PGA + M  10 iterations time:  417 us\n",
            "PGA + M  10 iterations time:  413 us\n",
            "PGA + M  500 iterations time:  19352 us\n",
            "Line Search 10 iterations time:  123705 us\n",
            "Manifold Optimization 10 iterations time:  31767 us\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    init_time = time.time()\n",
        "    _ = cnn_model.forward_cnn(next(iter(val_loader)))\n",
        "    final_time = time.time()\n",
        "    print('CNN 10 layers inference time: ', round(10**6*(final_time - init_time) / valid_size), 'us')\n",
        "\n",
        "\n",
        "    init_time = time.time()\n",
        "    _ = u_model.forward_unfolded(next(iter(val_loader)))\n",
        "    final_time = time.time()\n",
        "    print('U - PGA + M  10 iterations time: ', round(10**6*(final_time - init_time) / valid_size), 'us')\n",
        "\n",
        "    const_model.pga_iters = 10\n",
        "    train_loader, val_loader, _ = gen_data(train_size, valid_size, \"I.I.D.\")\n",
        "    init_time = time.time()\n",
        "    _ = const_model.forward_constant(next(iter(val_loader)))\n",
        "    final_time = time.time()\n",
        "    print('PGA + M  10 iterations time: ', round(10**6*(final_time - init_time)/valid_size), 'us')\n",
        "\n",
        "    const_model.pga_iters = 500\n",
        "    train_loader, val_loader, _ = gen_data(train_size, valid_size, \"I.I.D.\")\n",
        "    init_time = time.time()\n",
        "    _ = const_model.forward_constant(next(iter(val_loader)))\n",
        "    final_time = time.time()\n",
        "    print('PGA + M  500 iterations time: ', round(10**6*(final_time - init_time)/valid_size), 'us')\n",
        "\n",
        "\n",
        "    init_time = time.time()\n",
        "    _ = line_search(ls_iters = 10)\n",
        "    final_time = time.time()\n",
        "    print('Line Search 10 iterations time: ', round(10**6*(final_time - init_time)/valid_size), 'us')\n",
        "\n",
        "\n",
        "    init_time = time.time()\n",
        "    _ = manifold_optimization(MO_iters = 10)\n",
        "    final_time = time.time()\n",
        "    print('Manifold Optimization 10 iterations time: ', round(10**6*(final_time - init_time)/valid_size), 'us')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}